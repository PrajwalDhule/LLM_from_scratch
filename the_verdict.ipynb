{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LwFJgM3ICP4H"
   },
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import urllib.request\n",
    "import re\n",
    "from importlib.metadata import version\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- Reading the text file from the URL of the text file ----\n",
    "# url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "#       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "#       \"the-verdict.txt\")\n",
    "# file_path = \"the-verdict.txt\"\n",
    "# urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#    raw_text = f.read()\n",
    "# print(\"Total number of character:\", len(raw_text))\n",
    "# # print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- Preprocessing the text to split the text into tokens ----\n",
    "# preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "# preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "# print(len(preprocessed))\n",
    "# # print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- Building a vocabulary from the preprocessed (tokenised) text ----\n",
    "# all_words = sorted(set(preprocessed))\n",
    "# all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "# vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "# class SimpleTokenizerV2:\n",
    "#       def __init__(self, vocab):\n",
    "#             self.str_to_int = vocab\n",
    "#             self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "#       def encode(self, text):\n",
    "#             preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "#             preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "#             preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "#             ids = [self.str_to_int[s] for s in preprocessed]\n",
    "#             return ids\n",
    "\n",
    "#       def decode(self, ids):\n",
    "#             text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "#             text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "#             return text\n",
    "\n",
    "# # tokenizer = SimpleTokenizerV1(vocab)\n",
    "# # text =  \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "# # ids = tokenizer.encode(text)\n",
    "# # print(tokenizer.decode(ids))\n",
    "\n",
    "# text1 = \"Hello, do you like tea?\"\n",
    "# text2 = \"In the sunlit terraces of the palace.\"\n",
    "# text3 = \"painted.\"\n",
    "# text4 = \"pardonable.\"\n",
    "# text5 = \"lmao.\"\n",
    "# text = \" <|endoftext|> \".join((text1, text2, text3, text4, text5))\n",
    "# # print(text)\n",
    "# tokenizer = SimpleTokenizerV2(vocab)\n",
    "# # print(tokenizer.encode(text))\n",
    "# # print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Using tiktoken to implement BPE ----\n",
    "\n",
    "# print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "# tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "# text = (\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "#         \"of Akwirw ier.\" )\n",
    "# integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"}, disallowed_special=())\n",
    "# print(integers)\n",
    "# strings = tokenizer.decode(integers)\n",
    "# print(strings)\n",
    "\n",
    "# # ---- create input-target pairs ----\n",
    "# with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#       raw_text = f.read()\n",
    "\n",
    "# enc_text = tokenizer.encode(raw_text)\n",
    "# print(len(enc_text))\n",
    "\n",
    "# enc_sample = enc_text[50:]\n",
    "\n",
    "# context_size = 4\n",
    "# x = enc_sample[:context_size]\n",
    "# y = enc_sample[1:context_size+1]\n",
    "# print(f\"x: {x}\")\n",
    "# print(f\"y:      {y}\")\n",
    "\n",
    "# for i in range(1, context_size+1):\n",
    "#       context = enc_sample[:i]\n",
    "#       desired = enc_sample[i]\n",
    "#       print(context, \"---->\", desired)\n",
    "\n",
    "# for i in range(1, context_size+1):\n",
    "#       context = enc_sample[:i]\n",
    "#       desired = enc_sample[i]\n",
    "#       print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PyTorch dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# torch.__version__\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)   \n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):    \n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):   \n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating PyToch DataLoader using the GPTDatasetV1 class along with BPE tiktoken tokenizer \n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride = 128, shuffle = True, drop_last = True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")                        \n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,    \n",
    "        num_workers=num_workers    \n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922]])]\n"
     ]
    }
   ],
   "source": [
    "# basic example to use the dataloader\n",
    "\n",
    "# sometimes, takes a long time to execute (more than 1 minute, idk why)\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=4, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)     \n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "   raw_text, batch_size=8, max_length=max_length,\n",
    "  stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(4, 256)\n",
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n",
      "\n",
      "token embeddings\n",
      " tensor([[[-1.3940,  0.3517,  0.9450,  ..., -0.1821,  0.0900, -0.8099],\n",
      "         [ 0.9372, -1.5136,  1.0291,  ..., -0.0958, -0.5204, -0.0281],\n",
      "         [ 1.5280,  0.1011,  0.4999,  ...,  1.0641, -1.8279, -0.5470],\n",
      "         [ 1.2461, -0.0260, -1.3018,  ...,  0.1149, -1.5188, -0.0327]],\n",
      "\n",
      "        [[ 0.9334,  0.3306,  0.3520,  ...,  0.1425,  1.4388, -0.3138],\n",
      "         [ 2.2529, -1.2778, -0.3451,  ...,  0.4945, -0.9861,  1.0731],\n",
      "         [-0.0956, -0.8321, -0.8844,  ..., -0.4188,  0.5174, -1.0467],\n",
      "         [ 0.6763,  0.4832,  0.6521,  ..., -2.3425,  1.6773,  0.7270]],\n",
      "\n",
      "        [[ 0.9394, -0.3431, -2.4999,  ...,  2.0133, -0.6048,  0.9944],\n",
      "         [-0.7053, -0.5862,  1.2643,  ...,  0.5512, -1.3381, -1.1105],\n",
      "         [-0.5063, -1.3788,  0.2553,  ...,  0.1776,  0.9269, -0.8399],\n",
      "         [ 1.3182,  1.0709, -0.0928,  ..., -1.0938, -1.5077,  0.6416]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1704, -0.8800,  2.2553,  ..., -1.6184, -1.3620, -1.2494],\n",
      "         [-1.8969, -0.2128,  1.8694,  ...,  0.7972, -1.1338,  0.3753],\n",
      "         [-0.0597,  0.3429, -0.2071,  ..., -0.4097, -0.1791,  1.1263],\n",
      "         [ 0.0356,  0.2710, -0.1328,  ..., -0.3448,  0.0959,  2.1134]],\n",
      "\n",
      "        [[-0.4931,  1.9671,  0.9201,  ...,  2.0642, -0.2031,  0.1420],\n",
      "         [-0.3193, -0.6208, -0.4760,  ..., -0.1654, -0.9780, -0.2068],\n",
      "         [ 0.7049, -0.9431,  0.5993,  ..., -1.7874,  0.0646, -2.4740],\n",
      "         [ 0.5217, -0.1177, -1.6272,  ...,  1.5386,  1.7294,  0.9776]],\n",
      "\n",
      "        [[ 0.7049, -0.9431,  0.5993,  ..., -1.7874,  0.0646, -2.4740],\n",
      "         [-1.1341,  0.0208,  0.0090,  ...,  1.8848, -0.3831, -1.4383],\n",
      "         [ 0.9565, -0.8930, -0.8854,  ..., -1.6473, -2.5554,  0.5983],\n",
      "         [-0.3677, -0.1373,  0.0925,  ..., -0.7204,  0.9975, -0.3133]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "pos embeddings\n",
      " tensor([[-1.1900e+00,  7.8108e-01, -1.9824e+00,  ...,  1.5232e-01,\n",
      "          8.2897e-01,  1.3091e-01],\n",
      "        [ 6.1307e-01,  1.3464e-01,  6.4606e-01,  ...,  2.5635e-01,\n",
      "         -3.5445e-01,  8.6648e-01],\n",
      "        [-3.9309e-01, -1.2167e-01,  1.4176e-01,  ..., -1.0721e-03,\n",
      "          1.1186e+00,  3.5386e-01],\n",
      "        [-2.8822e-01,  3.7224e-01,  9.2285e-01,  ...,  3.4339e-01,\n",
      "         -7.0041e-02,  1.6847e+00]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "input embeddings\n",
      " tensor([[[-2.5840e+00,  1.1327e+00, -1.0374e+00,  ..., -2.9813e-02,\n",
      "           9.1896e-01, -6.7904e-01],\n",
      "         [ 1.5502e+00, -1.3789e+00,  1.6751e+00,  ...,  1.6056e-01,\n",
      "          -8.7482e-01,  8.3841e-01],\n",
      "         [ 1.1349e+00, -2.0551e-02,  6.4169e-01,  ...,  1.0631e+00,\n",
      "          -7.0924e-01, -1.9316e-01],\n",
      "         [ 9.5789e-01,  3.4624e-01, -3.7898e-01,  ...,  4.5833e-01,\n",
      "          -1.5888e+00,  1.6520e+00]],\n",
      "\n",
      "        [[-2.5660e-01,  1.1116e+00, -1.6304e+00,  ...,  2.9481e-01,\n",
      "           2.2678e+00, -1.8294e-01],\n",
      "         [ 2.8659e+00, -1.1431e+00,  3.0093e-01,  ...,  7.5083e-01,\n",
      "          -1.3405e+00,  1.9396e+00],\n",
      "         [-4.8870e-01, -9.5378e-01, -7.4259e-01,  ..., -4.1991e-01,\n",
      "           1.6361e+00, -6.9280e-01],\n",
      "         [ 3.8808e-01,  8.5542e-01,  1.5750e+00,  ..., -1.9992e+00,\n",
      "           1.6073e+00,  2.4117e+00]],\n",
      "\n",
      "        [[-2.5054e-01,  4.3796e-01, -4.4823e+00,  ...,  2.1656e+00,\n",
      "           2.2416e-01,  1.1253e+00],\n",
      "         [-9.2239e-02, -4.5161e-01,  1.9104e+00,  ...,  8.0759e-01,\n",
      "          -1.6926e+00, -2.4400e-01],\n",
      "         [-8.9934e-01, -1.5005e+00,  3.9710e-01,  ...,  1.7654e-01,\n",
      "           2.0455e+00, -4.8600e-01],\n",
      "         [ 1.0300e+00,  1.4431e+00,  8.3002e-01,  ..., -7.5044e-01,\n",
      "          -1.5777e+00,  2.3263e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0196e+00, -9.8890e-02,  2.7284e-01,  ..., -1.4661e+00,\n",
      "          -5.3305e-01, -1.1185e+00],\n",
      "         [-1.2838e+00, -7.8131e-02,  2.5155e+00,  ...,  1.0536e+00,\n",
      "          -1.4883e+00,  1.2418e+00],\n",
      "         [-4.5277e-01,  2.2125e-01, -6.5366e-02,  ..., -4.1074e-01,\n",
      "           9.3955e-01,  1.4801e+00],\n",
      "         [-2.5259e-01,  6.4328e-01,  7.9001e-01,  ..., -1.3746e-03,\n",
      "           2.5842e-02,  3.7981e+00]],\n",
      "\n",
      "        [[-1.6831e+00,  2.7482e+00, -1.0623e+00,  ...,  2.2165e+00,\n",
      "           6.2589e-01,  2.7292e-01],\n",
      "         [ 2.9378e-01, -4.8614e-01,  1.7011e-01,  ...,  9.0937e-02,\n",
      "          -1.3324e+00,  6.5969e-01],\n",
      "         [ 3.1181e-01, -1.0648e+00,  7.4109e-01,  ..., -1.7885e+00,\n",
      "           1.1833e+00, -2.1201e+00],\n",
      "         [ 2.3347e-01,  2.5455e-01, -7.0431e-01,  ...,  1.8820e+00,\n",
      "           1.6593e+00,  2.6623e+00]],\n",
      "\n",
      "        [[-4.8507e-01, -1.6207e-01, -1.3831e+00,  ..., -1.6351e+00,\n",
      "           8.9359e-01, -2.3431e+00],\n",
      "         [-5.2107e-01,  1.5539e-01,  6.5508e-01,  ...,  2.1411e+00,\n",
      "          -7.3760e-01, -5.7185e-01],\n",
      "         [ 5.6337e-01, -1.0146e+00, -7.4361e-01,  ..., -1.6484e+00,\n",
      "          -1.4368e+00,  9.5216e-01],\n",
      "         [-6.5590e-01,  2.3496e-01,  1.0153e+00,  ..., -3.7705e-01,\n",
      "           9.2748e-01,  1.3714e+00]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# For a GPT model’s absolute embedding approach, we just need to create another\n",
    "#  embedding layer that has the same embedding dimension as the token_embedding_\n",
    "#  layer:  The input to the pos_embeddings is usually a placeholder vector torch.arange(con\n",
    "# text_length), which contains a sequence of numbers 0, 1, ..., up to the maximum\n",
    "#  input length –1. The context_length is a variable that represents the supported input\n",
    "#  size of the LLM. Here,\n",
    "# For a GPT model’s  we choose it similar to the maximum length of the input text.\n",
    "#  In practice, input text can be longer than the supported context length, in which case\n",
    "#  we have to truncate the text.\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "print(pos_embedding_layer)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "print(\"\\ntoken embeddings\\n\", token_embeddings)\n",
    "print(\"\\npos embeddings\\n\", pos_embeddings)\n",
    "print(\"\\ninput embeddings\\n\", input_embeddings)\n",
    "\n",
    "# we now have the input embeddings that are used as input for the main LLM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env_global",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
