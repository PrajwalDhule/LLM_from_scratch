{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LwFJgM3ICP4H"
   },
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import urllib.request\n",
    "import re\n",
    "from importlib.metadata import version\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- Reading the text file from the URL of the text file ----\n",
    "# url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "#       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "#       \"the-verdict.txt\")\n",
    "# file_path = \"the-verdict.txt\"\n",
    "# urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#    raw_text = f.read()\n",
    "# print(\"Total number of character:\", len(raw_text))\n",
    "# # print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- Preprocessing the text to split the text into tokens ----\n",
    "# preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "# preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "# print(len(preprocessed))\n",
    "# # print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- Building a vocabulary from the preprocessed (tokenised) text ----\n",
    "# all_words = sorted(set(preprocessed))\n",
    "# all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "# vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "# class SimpleTokenizerV2:\n",
    "#       def __init__(self, vocab):\n",
    "#             self.str_to_int = vocab\n",
    "#             self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "#       def encode(self, text):\n",
    "#             preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "#             preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "#             preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "\n",
    "#             ids = [self.str_to_int[s] for s in preprocessed]\n",
    "#             return ids\n",
    "\n",
    "#       def decode(self, ids):\n",
    "#             text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "#             text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "#             return text\n",
    "\n",
    "# # tokenizer = SimpleTokenizerV1(vocab)\n",
    "# # text =  \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "# # ids = tokenizer.encode(text)\n",
    "# # print(tokenizer.decode(ids))\n",
    "\n",
    "# text1 = \"Hello, do you like tea?\"\n",
    "# text2 = \"In the sunlit terraces of the palace.\"\n",
    "# text3 = \"painted.\"\n",
    "# text4 = \"pardonable.\"\n",
    "# text5 = \"lmao.\"\n",
    "# text = \" <|endoftext|> \".join((text1, text2, text3, text4, text5))\n",
    "# # print(text)\n",
    "# tokenizer = SimpleTokenizerV2(vocab)\n",
    "# # print(tokenizer.encode(text))\n",
    "# # print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Using tiktoken to implement BPE ----\n",
    "\n",
    "# print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "# tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "# text = (\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "#         \"of Akwirw ier.\" )\n",
    "# integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"}, disallowed_special=())\n",
    "# print(integers)\n",
    "# strings = tokenizer.decode(integers)\n",
    "# print(strings)\n",
    "\n",
    "# # ---- create input-target pairs ----\n",
    "# with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#       raw_text = f.read()\n",
    "\n",
    "# enc_text = tokenizer.encode(raw_text)\n",
    "# print(len(enc_text))\n",
    "\n",
    "# enc_sample = enc_text[50:]\n",
    "\n",
    "# context_size = 4\n",
    "# x = enc_sample[:context_size]\n",
    "# y = enc_sample[1:context_size+1]\n",
    "# print(f\"x: {x}\")\n",
    "# print(f\"y:      {y}\")\n",
    "\n",
    "# for i in range(1, context_size+1):\n",
    "#       context = enc_sample[:i]\n",
    "#       desired = enc_sample[i]\n",
    "#       print(context, \"---->\", desired)\n",
    "\n",
    "# for i in range(1, context_size+1):\n",
    "#       context = enc_sample[:i]\n",
    "#       desired = enc_sample[i]\n",
    "#       print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PyTorch dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# torch.__version__\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)   \n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):    \n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):   \n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating PyToch DataLoader using the GPTDatasetV1 class along with BPE tiktoken tokenizer \n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride = 128, shuffle = True, drop_last = True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")                        \n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,    \n",
    "        num_workers=num_workers    \n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922]])]\n"
     ]
    }
   ],
   "source": [
    "# basic example to use the dataloader\n",
    "\n",
    "# sometimes, takes a long time to execute (more than 1 minute, idk why)\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=4, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)     \n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "   raw_text, batch_size=8, max_length=max_length,\n",
    "  stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# For a GPT model’s absolute embedding approach, we just need to create another\n",
    "#  embedding layer that has the same embedding dimension as the token_embedding_\n",
    "#  layer:  The input to the pos_embeddings is usually a placeholder vector torch.arange(con\n",
    "# text_length), which contains a sequence of numbers 0, 1, ..., up to the maximum\n",
    "#  input length –1. The context_length is a variable that represents the supported input\n",
    "#  size of the LLM. Here,\n",
    "# For a GPT model’s  we choose it similar to the maximum length of the input text.\n",
    "#  In practice, input text can be longer than the supported context length, in which case\n",
    "#  we have to truncate the text.\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "# we now have the input embeddings that are used as input for the main LLM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# 3-dimensional embeddings of the sequence “Your journey starts with one step”\n",
    "\n",
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts (x^3)\n",
    "   [0.22, 0.58, 0.33], # with (x^4)   \n",
    "   [0.77, 0.25, 0.10], # one (x^5)   \n",
    "   [0.05, 0.80, 0.55]] # step (x^6)   \n",
    ")\n",
    "\n",
    "# attention scores for 2nd input (\"journey\") wrt to all other inputs - using dot products\n",
    "# mutliplies element wise i.e. 1st ele with 1st ele, 2nd with 2nd and so on\n",
    "query = inputs[1]                           \n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)\n",
    "\n",
    "# normalize the attention scores to convert them to attention weights\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())\n",
    "\n",
    "# computing z(2) by multiplying the embedded input tokens, x(i), with the corresponding attention weights and then summing the resulting vectors\n",
    "query = inputs[1]        \n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8842, 1.2164, 1.2144, 0.6294, 0.8355, 0.6955])\n"
     ]
    }
   ],
   "source": [
    "# now with trainable weights\n",
    "# we compute 3 matrices: query Wq (for current input), key Wk, value Wv\n",
    "# taking 2nd input as query\n",
    "x_2 = inputs[1]    \n",
    "d_in = inputs.shape[1]     \n",
    "d_out = 2\n",
    "# Note that in GPT-like models, the input and output dimensions are usually the same, but to better follow the computation, we’ll use different input (d_in=3) and output (d_out=2) dimensions here.\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) \n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) \n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "attn_scores_2 = query_2 @ keys.T      \n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "env_global",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
