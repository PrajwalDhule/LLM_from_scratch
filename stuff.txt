progress:

start 3.5 (page 74 (96)) (EXPLORE ABOUT WHY of the stuff so far)
summary: start 3.4 (page 64 (86))
EXERCISE 3.1 (page 73 (95))

Notes:

- KEEP REFERRING TO SUMMARIES OF CHAPTERS

- Appendix A - a lot about PyTorch
- Transformers: has encoder and decoder 
- BERT focused more on the encoder part to excel at text completion tasks, GPT on decoder for text generation.
- zero shot: generalize to completely unseen tasks without any prior specific examples. 
- few-shot learning: involves learning from a minimal number of examples the user provides as input

- GPT models are pretrained on a relatively simple next-word prediction task
- Modern LLMs are trained in two main steps: 
  – First, they are pretrained on a large corpus of unlabeled text by using the prediction of the next word in a sentence as a label.
  – Then, they are fine-tuned on a smaller, labeled target dataset to follow instructions or perform classification tasks.
- LLMs are based on the transformer architecture. The key idea of the transformer  architecture is an attention mechanism that gives the LLM selective access
- A computational graph is a directed graph that allows us to express and visualize mathematical expressions.
- partial derivatives measure the rate at which a function changes with respect to one of its variables. 
- A gradient is a vector containing all of the partial derivatives of a multivariate function, a function with more than one variable as input.
- the chain rule is a way to compute gradients of a loss function given the model’s parameters in a computation graph. This provides the information needed to update each parameter to minimize the loss function.

- To get embeddings, we first need to tokenize all the words and assign them token IDs 
- we add a special token for new words not in a trained vocab. for eg: vocab: ("hello", id: 2), ("hi", id: 4).  text: "hello hi yo" -> "yo" will be treated as, say, a predefined token "<|unk|>" which we already add in the vocab during its creation.
- <|endoftext|> is also a commonly used special token to mark separation of sentences - usually assigned the largest token ID value.
- Byte-Pair Encoding (BPE) is commonly used to handle unknown words, more about BPE on page 34 (56) end, and GPT.
- While token embeddings provide consistent vector representations for each token, they lack a sense of the token’s position in a sequence. To rectify this, two main types of positional embeddings exist: absolute and relative. OpenAI’s GPT models utilize absolute positional embeddings, which are added to the token embedding vectors and are optimized during the model training.

-  In practice, we often use a third dataset, a so-called validation dataset, to find the opti
mal hyperparameter settings. A validation dataset is similar to a test set. However,
 while we only want to use a test set precisely once to avoid biasing the evaluation, we
 usually use the validation set multiple times to tweak the model settings. 

- ***(more should be read about 2.7 and 2.8 which isn't summarized here)***

- In self-attention, the “self” refers to the mechanism’s ability to compute attention
 weights by relating different positions within a single input sequence. It assesses and
 learns the relationships and dependencies between various parts of the input itself,
 such as words in a sentence or pixels in an image. 
-This is in contrast to traditional attention mechanisms, where the focus is on the rela
tionships between elements of two different sequences, such as in sequence-to
sequence models where the attention might be between an input sequence and an
 output sequence, such as the example depicted in figure 3.5. 


- For example, consider an input text like “Your journey starts with one step.” In this
 case, each element of the sequence, such as x(1), corresponds to a d-dimensional
 embedding vector representing a specific token, like “Your.” Figure 3.7 shows these
 input vectors as three-dimensional embeddings.
- In self-attention, our goal is to calculate context vectors z(i) for each element x(i)
 in the input sequence. A context vector can be interpreted as an enriched embedding
 vector.
- To illustrate this concept, let’s focus on the embedding vector of the second input
 element, x(2) (which corresponds to the token “journey”), and the corresponding con
text vector, z(2), shown at the bottom of figure 3.7. This enhanced context vector, z(2),
 is an embedding that contains information about x(2) and all other input elements,
 x(1) to x(T)

 - The goal of self-attention is to compute a context vector for each input 
element that combines information from all other input elements. In this example, 
we compute the context vector z(2). The importance or contribution of each input 
element for computing z(2) is determined by the attention weights 21 to 2T. When 
computing z(2), the attention weights are calculated with respect to input element 
x(2) and all other inputs. (fig 3.7)
- The context vector z(2) is computed as a combination of all input vectors weighted with respect to input element x(2)
- In the context of self-attention mechanisms, the dot product determines the extent to which
 each element in a sequence focuses on, or “attends to,” any other element: the
 higher the dot product, the higher the similarity and attention score between two
 elements.
- to summarize: attention scores (n) -> attention weights (n) -> context vector (m)
  no of dimensions: n = no of input tokens, m = no of dimensions
- multiplying embedding vectors with each other (to get attention scores) and other similar operations can simplified and made efficient using matrix multpln
- 

- KEEP REFERRING TO SUMMARIES OF CHAPTERS




# import os
# import tempfile
# cache_dir = os.path.join(tempfile.gettempdir(), "data-gym-cache") 
# print(cache_dir)